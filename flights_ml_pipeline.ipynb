{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c2dc0f",
   "metadata": {},
   "source": [
    "## Building a Machine Learning Pipeline with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ccf2d3",
   "metadata": {},
   "source": [
    "model to predict flight lateness based on various features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f76cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "327014e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d57e6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001CD24812220>\n"
     ]
    }
   ],
   "source": [
    "# Creating SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Print spark\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe41243b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Checking for existing tables in the catalog\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce9d4a",
   "metadata": {},
   "source": [
    "Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52467999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the flight data\n",
    "flight_df = spark.read.csv('flights_small.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be80abb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='flights', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "# Creating table called flights to support SQL queries\n",
    "flight_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Checking for existing tables in the catalog\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b83e26",
   "metadata": {},
   "source": [
    "Data Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd3510ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|\n",
      "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|\n",
      "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query\n",
    "query = \"SELECT * FROM flights LIMIT 10\"\n",
    "\n",
    "# Get the first 10 rows of flights\n",
    "flights10 = spark.sql(query)\n",
    "\n",
    "# Show the results\n",
    "flights10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c77f4469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame: (10000, 16)\n"
     ]
    }
   ],
   "source": [
    "def dataframe_shape(df):\n",
    "  \"\"\"\n",
    "  This function takes a Spark DataFrame and returns its shape as a tuple\n",
    "  containing the number of rows and columns.\n",
    "\n",
    "  Args:\n",
    "      df: The DataFrame to get the shape of.\n",
    "\n",
    "  Returns:\n",
    "      str: the number of rows and columns.\n",
    "  \"\"\"\n",
    "  rows = df.count()\n",
    "  columns = len(df.columns)\n",
    "  shape = print(\"Shape of DataFrame: ({}, {})\".format(rows, cols))\n",
    "  return shape\n",
    "\n",
    "dataframe_shape(flight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3e107d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame: (10000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Getting number of rows and columns\n",
    "f_rows = flight_df.count()\n",
    "f_columns = len(flight_df.columns)\n",
    "\n",
    "# Printing shape of DataFrame\n",
    "print(\"Shape of DataFrame: ({}, {})\".format(f_rows, f_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9d124d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "def count_duplicates(df):\n",
    "  \"\"\"\n",
    "  This function counts the number of duplicate rows in a PySpark DataFrame.\n",
    "\n",
    "  Args:\n",
    "      df (pyspark.sql.DataFrame): The DataFrame to check for duplicates.\n",
    "\n",
    "  Returns:\n",
    "      str: The number of duplicate rows found.\n",
    "  \"\"\"\n",
    "  num_duplicates = df.count() - df.dropDuplicates().count()\n",
    "  dups = print(\"Number of duplicate rows:\", num_duplicates)\n",
    "  return dups\n",
    "\n",
    "# Checking the number of duplicate rows\n",
    "count_duplicates(flight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77127b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+------------+----+---+---+\n",
      "|faa|                name|       lat|         lon| alt| tz|dst|\n",
      "+---+--------------------+----------+------------+----+---+---+\n",
      "|04G|   Lansdowne Airport|41.1304722| -80.6195833|1044| -5|  A|\n",
      "|06A|Moton Field Munic...|32.4605722| -85.6800278| 264| -5|  A|\n",
      "|06C| Schaumburg Regional|41.9893408| -88.1012428| 801| -6|  A|\n",
      "|06N|     Randall Airport| 41.431912| -74.3915611| 523| -5|  A|\n",
      "|09J|Jekyll Island Air...|31.0744722| -81.4277778|  11| -4|  A|\n",
      "|0A9|Elizabethton Muni...|36.3712222| -82.1734167|1593| -4|  A|\n",
      "|0G6|Williams County A...|41.4673056| -84.5067778| 730| -5|  A|\n",
      "|0G7|Finger Lakes Regi...|42.8835647| -76.7812318| 492| -5|  A|\n",
      "|0P2|Shoestring Aviati...|39.7948244| -76.6471914|1000| -5|  U|\n",
      "|0S9|Jefferson County ...|48.0538086|-122.8106436| 108| -8|  A|\n",
      "+---+--------------------+----------+------------+----+---+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading in the airports data\n",
    "airports_df = spark.read.csv('airports.csv', header=True)\n",
    "\n",
    "# Show the data\n",
    "airports_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d19e860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame: (1397, 16)\n"
     ]
    }
   ],
   "source": [
    "# Printing shape of DataFrame\n",
    "dataframe_shape(airports_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a049458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Checking the number of duplicate rows\n",
    "count_duplicates(airports_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ae28b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+----------------+--------+-------+-----+-----+---------+\n",
      "|tailnum|year|                type|    manufacturer|   model|engines|seats|speed|   engine|\n",
      "+-------+----+--------------------+----------------+--------+-------+-----+-----+---------+\n",
      "| N102UW|1998|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N103US|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N104UW|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N105UW|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N107US|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N108UW|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N109UW|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N110UW|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N111US|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N11206|2000|Fixed wing multi ...|          BOEING| 737-824|      2|  149|   NA|Turbo-fan|\n",
      "+-------+----+--------------------+----------------+--------+-------+-----+-----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading in the plane data\n",
    "planes_df = spark.read.csv('planes.csv', header=True)\n",
    "\n",
    "# Show the data\n",
    "planes_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a40b6d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame: (2628, 16)\n"
     ]
    }
   ],
   "source": [
    "# Printing shape of DataFrame\n",
    "dataframe_shape(planes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0be79fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Checking the number of duplicate rows\n",
    "count_duplicates(planes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b471dab",
   "metadata": {},
   "source": [
    "#### ML PIPELINE for model to predict lateness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e445a7e5",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4afcd2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename year column for ease of joining\n",
    "planes_df = planes_df.withColumnRenamed(\"year\", \"plane_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1692ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the DataFrames\n",
    "model_data = flight_df.join(planes_df, on=\"tailnum\", how=\"leftouter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2fc5cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the columns to integers\n",
    "model_data = model_data.withColumn(\"arr_delay\", model_data.arr_delay.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"air_time\", model_data.air_time.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"month\", model_data.month.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "79847e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the column plane_age\n",
    "model_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b05a459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create is_late\n",
    "model_data = model_data.withColumn(\"is_late\", model_data.arr_delay > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e4bb1e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to an integer\n",
    "model_data = model_data.withColumn(\"label\", model_data.is_late.cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "608c7146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values\n",
    "model_data = model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc93aaf",
   "metadata": {},
   "source": [
    "Feature Engineering for Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "62d87a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create a StringIndexer\n",
    "carr_indexer = StringIndexer(inputCol=\"carrier\", outputCol=\"carrier_index\")\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "carr_encoder = OneHotEncoder(inputCol=\"carrier_index\", outputCol=\"carrier_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ce84c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIndexer\n",
    "dest_indexer = StringIndexer(inputCol= \"dest\", outputCol= \"dest_index\")\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "dest_encoder = OneHotEncoder(inputCol= \"dest_index\", outputCol = \"dest_fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f47b57",
   "metadata": {},
   "source": [
    "Building the Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d936bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols= [\"month\", \"air_time\", \"carrier_fact\", \"dest_fact\", \"plane_age\"], outputCol= \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "62653ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Make the pipeline\n",
    "flights_pipe = Pipeline(stages= [dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ae266f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the data\n",
    "piped_data = flights_pipe.fit(model_data).transform(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc879a3",
   "metadata": {},
   "source": [
    "Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "686c9dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "training, test = piped_data.randomSplit([.6, .4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53d13bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression Estimator\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fef33e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluation submodule\n",
    "import pyspark.ml.evaluation as evals\n",
    "\n",
    "# Create a BinaryClassificationEvaluator\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "124ff9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tuning submodule\n",
    "import numpy as np\n",
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "# Create the parameter grid\n",
    "grid = tune.ParamGridBuilder()\n",
    "\n",
    "# Add the hyperparameter\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
    "\n",
    "# Build the grid\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fca3e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CrossValidator\n",
    "cv = tune.CrossValidator(estimator=lr,\n",
    "               estimatorParamMaps=grid,\n",
    "               evaluator=evaluator\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0d57d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit cross validation models\n",
    "models = cv.fit(training)\n",
    "\n",
    "# Extract the best model\n",
    "best_lr = models.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1642b11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel: uid=LogisticRegression_304443256e82, numClasses=2, numFeatures=81\n"
     ]
    }
   ],
   "source": [
    "# Call lr.fit()\n",
    "best_lr = lr.fit(training)\n",
    "\n",
    "# Print best_lr\n",
    "print(best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eb2eb8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6845499411127688\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict the test set\n",
    "test_results = best_lr.transform(test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(evaluator.evaluate(test_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcfeee1",
   "metadata": {},
   "source": [
    "model's AUC is 0.685"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
